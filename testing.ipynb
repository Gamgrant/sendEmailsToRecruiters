{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# conda create --name pipeline_testing python=3.10 -y\n",
    "# conda activate pipeline_testing\n",
    "# conda install -c conda-forge streamlit pandas gspread requests -y\n",
    "# pip install google-auth google-auth-oauthlib google-auth-httplib2 openai python-dotenv\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import gspread\n",
    "import os\n",
    "import requests\n",
    "from google.oauth2.service_account import Credentials\n",
    "from openai import OpenAI\n",
    "import config\n",
    "import time\n",
    "\n",
    "print('All packages installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test if you are connected to spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection to the spreadsheet\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "SERVICE_ACCOUNT_FILE = config.SERVICE_ACCOUNT_FILE\n",
    "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(creds)\n",
    "SPREADSHEET_ID = config.SPREADSHEET_ID\n",
    "try:\n",
    "    sheet = client.open_by_key(SPREADSHEET_ID).sheet1\n",
    "    data = sheet.get_all_records()\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if you can make simple api call to openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "job_desc= \"\"\"About GitHubAs the global home for all developers, GitHub is the complete AI-powered developer platform to build, scale, and deliver secure software. Over 150+ million developers, including more than 90% of the Fortune 100 companies, use GitHub to collaborate and experiment across 420+ million repositories. repositories. With all the collaborative features of GitHub, it has never been easier for individuals and teams to write faster, better code.\n",
    "LocationsIn this role you can work from Remote, United States\n",
    "Overview\n",
    "As a software engineer at GitHub, you will enhance the collaboration experience at GitHub by working closely with a community of engineers and designers with a distributed, diverse and passionate team delivering the services that millions of developers depend on. In this role you will design, prototype, implement, ship and support highly performant and inspiring user experiences with your team.\n",
    "\n",
    "We are looking for creative problem solvers and diverse thinkers, people who care about culture as well as customers and features. We believe that how we do things is as important as what we do. Big vision, a common purpose, passion for quality, curiosity, dedication, and investment in fun and collaboration are what lead to great results. Great products reflect the teams that build them.\n",
    "Responsibilities\n",
    "• Design, develop, test and ship high-quality technical solutions that scale across multiple GitHub services.\n",
    "• Collaborate with cross-functional teams to define and implement innovative solutions.\n",
    "• Receive technical mentorship, collaborate and pair with others, provide code reviews, and communicate clearly to your leadership about progress, blockers, and gaps.\n",
    "• Own and advocate for the health and quality of the systems that the team builds, including participating in on-call and first responder rotations\n",
    "• Write architecture briefs and proposals, carry out code experiments, and build prototypes to learn how we can achieve planetary scale with our systems.\n",
    "• Design and implement APIs to facilitate seamless integration between software components.\n",
    "• Utilize CI/CD tools to set up automated pipelines for continuous integration and delivery.\n",
    "• Become intimately familiar with the systems you build and take pride in writing maintainable code.\n",
    "Qualifications\n",
    "Required Qualifications\n",
    "• 1+ year(s) experience in Software Engineering, Computer Science, or related technical discipline with proven experience maintaining production software in languages including, but not limited to, C, C++, C#, Java, JavaScript, Go, Ruby, Rust, or Python\n",
    "    ◦  OR Associate’s Degree in Computer Science, Electrical Engineering, Electronics Engineering, Math, Physics, Computer Engineering, or related field\n",
    "    ◦ OR equivalent experience.\n",
    "Preferred Qualifications\n",
    "• Experience with Git and GitHub\n",
    "• Experience using Azure technologies is a bonus\n",
    "• Experience using front-end frameworks like React.js\n",
    "• Understanding of REST principles and experience with backend APIs\n",
    "• Strong written and verbal communication skill\n",
    "• Passionate about healthy team culture and collaboration\n",
    "• Comfortable working transparently in an agile environment and soliciting feedback from peers\n",
    "Compensation RangeThe base salary range for this job is USD $66,900.00 - USD $177,600.00 /Yr.These pay ranges are intended to cover roles based across the United States. An individual's base pay depends on various factors including geographical location and review of experience, knowledge, skills, abilities of the applicant. At GitHub certain roles are eligible for benefits and additional rewards, including annual bonus and stock. These rewards are allocated based on individual impact in role. In addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee's role.\n",
    "GitHub Leadership Principles\n",
    "GitHub values\n",
    "• Customer-obsessed\n",
    "• Ship to learn\n",
    "• Growth mindset\n",
    "• Own the outcome\n",
    "• Better together\n",
    "• Diverse and inclusive\n",
    "Manager fundamentals\n",
    "• Model\n",
    "• Coach\n",
    "• Care\n",
    "Leadership principles\n",
    "• Create clarity\n",
    "• Generate energy\n",
    "• Deliver success\n",
    "Who We AreGitHub is the world’s leading AI-powered developer platform with 150 million developers and counting. We’re also home to the biggest open-source community on earth (and 99% of the world’s software has open-source code in its DNA). Many of the apps and programs you use every day are built on GitHub.Our teams are dreamers, doers, and pioneers, leading the way in AI, driving humanitarian efforts around the globe, and even sending open source to Mars (and beyond!).At GitHub, our goal is to create the space you need to do your best work. We’re remote-first and offer competitive pay, generous learning and growth opportunities, and excellent benefits to support you, wherever you are—because we know that people flourish when they can work on their own terms.Join us, and let’s change the world, together.\n",
    "EEO StatementGitHub is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people of all walks of life. We don't discriminate against employees or applicants based on gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, disability, pregnancy status, veteran status, or any other differences. Also, if you have a disability, please let us know if there's any way we can make the interview process better for you; we're happy to accommodate!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    Given the following job description and my resume details, generate a short summary of how my skills align with the role.\n",
    "\n",
    "    Job Description:\n",
    "    {job_desc}\n",
    "\n",
    "    My Resume Details:\n",
    "    GRANT OVSEPYAN\n",
    "    EDUCATION\n",
    "    Carnegie Mellon University September 2023 - December 2024\n",
    "    Master's, Artificial Intelligence Engineering GPA: 3.81\n",
    "    University of Wisconsin - Madison August 2018 - August 2023\n",
    "    Bachelor's, Computer Science\n",
    "    Bachelor's, Chemical Engineering GPA: 3.6\n",
    "    PROFESSIONAL EXPERIENCE\n",
    "    GE Healthcare Waukesha, WI, USA\n",
    "    Software Development Intern May 2024 - August 2024\n",
    "    • Replaced cpprest JSON with a custom serialization protocol across ~200 files, implementing multithreading to enhance processing speed\n",
    "    and security for cross-platform, multi-language support.\n",
    "    • Developed automated testing pipelines utilizing Snowflake OLAP database to process and validate large volumes of MRI scan data\n",
    "    across multiple platforms, including Windows, Linux, and macOS.\n",
    "    Mandli Communications, Inc. Madison, WI, USA\n",
    "    Software Development Intern January 2023 - July 2023\n",
    "    • Fine-tuned CLRNet lane detection model with custom feature extractors on proprietary dataset, integrating CULane pre-training and\n",
    "    GPU-profiled training pipeline to achieve 15% accuracy improvement and 25% reduced inference latency.\n",
    "    • Architected real-time LiDAR streaming system processing 1.4M points/sec using ROS2 rclcpp for pub/sub nodes and WebSocket\n",
    "    bridge, implementing concurrent data handling with mutex locks for synchronized 40-60 Mbps throughput across distributed\n",
    "    navigation systems.\n",
    "    Additive Manufacturing Laboratory Madison, WI, USA\n",
    "    Research Assistant January 2019 - March 2023\n",
    "    • Improved printing precision 100x using MATLAB COMSOL API for thermal simulation-driven topology optimization.\n",
    "    • Designed a user interface to enhance usability and published results at Nature Communications (DOI: 10.1038/s41467-024-48159-7).\n",
    "    • Created firmware and slicing software for multi-material 3D printing, ensuring precise G-code execution and enhanced accuracy.\n",
    "    Koch Industries: Flint Hills Resources Rosemount, MN, USA\n",
    "    Data Science Intern May 2022 - August 2022\n",
    "    • Fine-tuned and adapted 60+ machine learning models for equipment analysis, deploying data-driven optimizations to enhance\n",
    "    predictive accuracy and operational efficiency. Enabled field engineers in the crude unit to make critical, data-backed decisions.\n",
    "    • Used Facebook Prophet and Seeq API for advanced data analytics, boosting forecasting accuracy and reducing processing time by 23%.\n",
    "    Software Development Freelance\n",
    "    Stock Price Prediction Platform\n",
    "    • Developed a full-stack stock tracking app with React Native & Appwrite, integrating Alpaca API to authenticate users and\n",
    "    dynamically sync their portfolios, optimizing API usage with cached stock data.\n",
    "    • Built a Flask backend on DigitalOcean using Nginx & Gunicorn, implementing a cron job to fetch & store stock prices for all\n",
    "    user-owned companies in Appwrite DB, reducing redundant API calls.\n",
    "    • Engineered a modified LSTM model for stock price prediction, incorporating historical market trends & trading volume,\n",
    "    optimized with low-latency inference for real-time forecasting via a Flask API.\n",
    "    • Implemented news sentiment-driven stock fluctuation detection by extracting Alpaca’s news feed API, analyzing sentiment\n",
    "    with a ChatGPT-informed Graph Neural Network, and flagging potential volatility triggers.\n",
    "    Full-Stack Image Processing / Next.js, TypeScript, AWS Lambda, Gateway, S3, DynamoDB\n",
    "    • Developed web app with real-time background removal using Next.js, TypeScript, and AWS Lambda. Integrated Amazon S3\n",
    "    with pre-signed URLs, triggering Lambda functions to invoke the remove.bg API for image processing.\n",
    "    • Employed OpenCV for contour detection and Pillow for dynamic text wrapping on processed images. Leveraged DynamoDB for logging\n",
    "    metadata, ensuring scalable cloud-based architecture and optimized performance.\n",
    "    Full-Stack Muscle App / React Native, Expo, Nativewind, Animatable, Appwrite\n",
    "    • Developed a Muscle App to analyze workout routines and provide personalized recommendations for optimal muscle growth.\n",
    "    • Implemented a robust authentication system, dynamic home screen, full-text search, and tab navigation for seamless user experience.\n",
    "    Running WASM3 in the Linux kernel\n",
    "    • Developed a Linux kernel module that dynamically loads and executes WebAssembly (WASM) bytecode from user space using the\n",
    "    Wasm3 interpreter, enabling modular and efficient in-kernel processing.\n",
    "    • Implemented an ioctl-based interface to securely handle communication between user-space applications and the kernel module,\n",
    "    +1 (608) 216-5392 |\n",
    "    grovse1999@gmail.com |\n",
    "    linkedin.com/in/grant-ovs/ |\n",
    "    github.com/Gamgrant |\n",
    "    facilitating the transmission of WASM binaries and execution commands.\n",
    "    • Exposed native kernel functions to WASM modules by binding functions through Wasm3's API, ensuring robust memory management\n",
    "    Data Science Freelance\n",
    "    FIFA Performance Prediction Model /Python, PySpark, PostgreSQL\n",
    "    • Designed and implemented cloud-based architecture of GCP managing PostgreSQL database for efficient handling of FIFA player\n",
    "    statistics using complex joins, window functions, and transaction control for performance metrics processing\n",
    "    • Enhanced system scalability through PySpark, utilizing BigQuery and Dataflow for batch/stream processing, while parallelizing\n",
    "    predictive models (SVM, Random Forest) with automated hyperparameter tuning via Spark MLlib for improved processing efficiency.\n",
    "    Audio-Guided Album Cover Art Generation / PyTorch, Python\n",
    "    • Conditioned Stable Diffusion on CLIP for text, integrated ControlNet, Cross Attention for audio features to generate album cover art.\n",
    "    • Used YouTube, Wikipedia and Genius, as well as BeautifulSoup, requests, pandas, numpy and urllib packages and multithreading to\n",
    "    • extract relevant data for scraping music albums from 1980 to 2024 using .csv/.json formats (ETL).\n",
    "    • Set-up Spotify API for data preprocessing, extracting audio features, album genres and cover art for input in generative model.\n",
    "    RAG System using LLaMA-3, Mistral, and DeepSeek LLMs / Python,LangChain, Pinecone\n",
    "    • Automated data scraping using Selenium for dynamic web pages and expandable content, capturing multi-level embedded data.\n",
    "    • Extracted structured data from static HTML using BeautifulSoup, Requests, and lxml, employing XPath for precise parsing.\n",
    "    • Used Pinecone's multilingual-e5-large embeddings and Chroma vector store, enabling efficient vector similarity search for retrieval.\n",
    "    • Incorporated retrieved context in prompts using LangChain with LLaMA-3, Mistral, DeepSeek models, leading to significant accuracy\n",
    "    improvements over baseline models without retrieval augmentation.\n",
    "    Machine Learning Freelance\n",
    "    Program Selection Using Code Execution Agents / Python, CodeT NLP Lead\n",
    "    • Re-implemented the CodeT algorithm to improve program selection accuracy in code generation tasks, achieving significant\n",
    "    • improvements in pass@1 and pass@k scores on HumanEval and MBPP benchmarks.\n",
    "    • Conducted a comparative study on state-of-the-art methods for program selection, including MBR-Exec and Reflexion, highlighting\n",
    "    their limitations in handling syntactically plausible but functionally incorrect code.\n",
    "    • Proposed an enhancement to CodeT using Bayesian inference for more informative test-case selection, aimed at increasing the likelihood\n",
    "    • of identifying functionally accurate programs using CodeLLaMA-13b and Qwen-2.5-Coder-7B.\n",
    "    Advanced Projects in Trustworthy AI for Autonomous Systems\n",
    "    • Led rigorous evaluations and enhancements of perception models within the \"Trustworthy AI\" course, focusing on autonomous driving\n",
    "    applications using SafeBench and CARLA simulator.\n",
    "    • Implemented and analyzed object detection metrics for cutting-edge models including YOLOv5 and Faster R-CNN, enhancing detection\n",
    "    performance under varied conditions.\n",
    "    • Designed and tested adversarial patches to assess robustness of AI models against simulated real-world adversarial conditions, utilizing\n",
    "    precision-recall curves and average precision metrics across multiple IoU thresholds.\n",
    "    • Configured and managed cloud-based AI systems on AWS EC2, optimized GPU setups, and facilitated real-time simulations and\n",
    "    debugging via remote desktop environments with TurboVNC.\n",
    "    MiniLLAMA / PyTorch, Python\n",
    "    • Implemented fine-tuning of LLaMA, utilizing gradient accumulation and mixed precision (fp16) to reduce memory consumption.\n",
    "    • Leveraged PyTorch Lightning for distributed training, integrating DataParallel and ModelParallel strategies.\n",
    "    • Applied LoRA for efficient fine-tuning, reducing the number of trainable parameters while maintaining high performance.\n",
    "    • Tuned optimizers (AdamW, Adafactor) and learning rate schedules (Cosine Annealing, Linear Warmup) to balance computation and\n",
    "    convergence speed.\n",
    "    SKILLS\n",
    "    • Programming Languages: Python, Java, C++, SQL, Bash, JavaScript, TypeScript, Scala, Swift, R, Julia, MATLAB\n",
    "    • Frameworks & Libraries: TensorFlow, PyTorch, scikit-learn, Keras, NLTK, OpenCV, Pandas, NumPy, PySpark, Hugging Face,\n",
    "    Flask, React Native, Next.js, Firebase\n",
    "    • Cloud & DevOps: AWS (SageMaker, Lambda, DynamoDB, S3, API Gateway), GCP (BigQuery, Dataflow, AI Platform), Azure\n",
    "    ML, Docker, Kubernetes, Jenkins, Git, CI/CD\n",
    "    • Databases: MySQL, PostgreSQL, MongoDB, DynamoDB, Snowflake, Bigtable\n",
    "    • Big Data & Distributed Systems: Hadoop, Apache Spark, Apache Kafka\n",
    "    • Tools & Software: AutoCAD, MATLAB COMSOL, PTC Creo, Tableau, Power BI, Minitab\n",
    "\n",
    "    Output should be a concise, professional summary continuing from: 'Grant has an experience in ...'. \n",
    "    The response should be **a single sentence** with a maximum of 14 words, but do not include \"Grant has an experience in\" in your response, include what comes after it.\n",
    "    Your goal is NOT to overgeneralize like saying (\"software engineering, collaborating on cross-functional teams, and implementing scalable, high-quality solutions\"), but rather \n",
    "    include the actuall technical skills, languages and frameworks. be very detailed and technically dense\"\"\"\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "try:\n",
    "    # Initialize client\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # API request\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract response text\n",
    "    response_text = completion.choices[0].message.content.strip()\n",
    "    print(response_text)\n",
    "except Exception:\n",
    "    # fetching output from openAI failed\n",
    "    print(\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
